{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install requrements**"
      ],
      "metadata": {
        "id": "H1mt8ulabHPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  transformers datasets"
      ],
      "metadata": {
        "id": "oqGWCDIAaxt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **import libraries**"
      ],
      "metadata": {
        "id": "E_XYg1nobRgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "import numpy as np\n",
        "import torch\n",
        "import math"
      ],
      "metadata": {
        "id": "MGMhmC5MbVYx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and split the dataset**"
      ],
      "metadata": {
        "id": "SyNyBMcmbeMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and split the dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "train_test_split = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "test_dataset = train_test_split[\"test\"]\n"
      ],
      "metadata": {
        "id": "woo0RiJWbeq9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialize tokenizer**"
      ],
      "metadata": {
        "id": "Ly-UZwc-bwPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# Add padding token to the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "CqjcY_ppbwi1"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to tokenize the text\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize the text and return input_ids, attention_mask, and labels\n",
        "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "9P8oKOM8cCV1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to the datasets\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "Sg-dq3U5cP7V"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load model**"
      ],
      "metadata": {
        "id": "rkyxlxtkdBc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "ewosq6vkcmSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **training arguments**"
      ],
      "metadata": {
        "id": "75G-Fyq8eaVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "i-fvwtbygIz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] -U"
      ],
      "metadata": {
        "id": "iKXz4kn3gjIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show accelerate"
      ],
      "metadata": {
        "id": "3smdSE7yhAwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",           # Directory to save the model\n",
        "    eval_strategy=\"steps\",            # Evaluate the model every 'eval_steps'\n",
        "    save_strategy=\"steps\",            # Save the model every 'save_steps'\n",
        "    learning_rate=5e-5,               # Learning rate for training\n",
        "    per_device_train_batch_size=4,    # Batch size for training\n",
        "    per_device_eval_batch_size=4,     # Batch size for evaluation\n",
        "    num_train_epochs=3,               # Number of training epochs\n",
        "    weight_decay=0.01,                # Weight decay for regularization\n",
        "    logging_dir='./logs',             # Directory for storing logs\n",
        "    logging_steps=10,                 # Log every 'logging_steps'\n",
        "    eval_steps=500,                   # Evaluate every 'eval_steps'\n",
        "    save_steps=500,                   # Save every 'save_steps'\n",
        "    save_total_limit=2,               # Limit the total amount of checkpoints\n",
        "    load_best_model_at_end=True,      # Load the best model at the end of training\n",
        "    metric_for_best_model=\"loss\",     # The metric to use to compare models\n",
        "    greater_is_better=False           # Whether a greater metric is better\n",
        ")\n"
      ],
      "metadata": {
        "id": "AiAiO3--eZgr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metrics**"
      ],
      "metadata": {
        "id": "FltUfYI6kBR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (predictions == labels).mean()\n",
        "\n",
        "    # Calculate perplexity\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "    shift_labels = labels[..., 1:].contiguous()\n",
        "    loss_fct = torch.nn.CrossEntropyLoss()\n",
        "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "    perplexity = math.exp(loss)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"perplexity\": perplexity,\n",
        "        \"loss\": loss.item()\n",
        "    }"
      ],
      "metadata": {
        "id": "n44TOoaGkI4S"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trainer**"
      ],
      "metadata": {
        "id": "BmZcZnUrhx3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DataCollatorWithPadding to handle dynamic padding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                          # The model to train\n",
        "    args=training_args,                   # Training arguments\n",
        "    train_dataset=tokenized_train_dataset,  # Training dataset\n",
        "    eval_dataset=tokenized_test_dataset,  # Evaluation dataset\n",
        "    compute_metrics=compute_metrics,      # Custom metrics\n",
        "    data_collator=data_collator           # Dynamic padding\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "ucYRx-Omhxqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "HnwTUnyZkN8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation results:\", results)\n"
      ],
      "metadata": {
        "id": "jPI4sdoijD3V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}