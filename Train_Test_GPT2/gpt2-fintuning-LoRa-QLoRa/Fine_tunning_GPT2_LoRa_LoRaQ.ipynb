{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install  transformers datasets"
      ],
      "metadata": {
        "id": "KZfJOxNOmpz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "i-fvwtbygIz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] -U"
      ],
      "metadata": {
        "id": "iKXz4kn3gjIv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show accelerate"
      ],
      "metadata": {
        "id": "3smdSE7yhAwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "PLOIkiA6GmgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "    dataset = dataset[\"train\"].train_test_split(test_size=0.01)[\"train\"]\n",
        "    train_test_split = dataset.train_test_split(test_size=0.01)\n",
        "    train_dataset = train_test_split[\"train\"]\n",
        "    test_dataset = train_test_split[\"test\"]\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=32)\n",
        "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "        return tokenized\n",
        "\n",
        "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    return tokenized_train_dataset, tokenized_test_dataset, tokenizer\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    logits = torch.tensor(logits)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    accuracy = np.mean(predictions == labels.numpy())\n",
        "\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "    shift_labels = labels[..., 1:].contiguous()\n",
        "    loss_fct = torch.nn.CrossEntropyLoss()\n",
        "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "    perplexity = math.exp(loss.item())\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"perplexity\": perplexity,\n",
        "        \"loss\": loss.item()\n",
        "    }\n",
        "\n",
        "class LoRaLayer(torch.nn.Module):\n",
        "    def __init__(self, weight, rank):\n",
        "        super(LoRaLayer, self).__init__()\n",
        "        self.rank = rank\n",
        "        self.weight = weight\n",
        "        if len(weight.size()) != 2:\n",
        "            raise ValueError(f\"Expected weight tensor of dimension 2, but got {len(weight.size())}\")\n",
        "        self.lora_a = torch.nn.Parameter(torch.zeros(weight.size(0), rank))\n",
        "        self.lora_b = torch.nn.Parameter(torch.zeros(rank, weight.size(1)))\n",
        "        torch.nn.init.kaiming_uniform_(self.lora_a)\n",
        "        torch.nn.init.zeros_(self.lora_b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.mm(x, self.weight + torch.mm(self.lora_a, self.lora_b))\n",
        "\n",
        "class QLoRaLayer(torch.nn.Module):\n",
        "    def __init__(self, weight, rank):\n",
        "        super(QLoRaLayer, self).__init__()\n",
        "        self.rank = rank\n",
        "        self.weight = weight\n",
        "        if len(weight.size()) != 2:\n",
        "            raise ValueError(f\"Expected weight tensor of dimension 2, but got {len(weight.size())}\")\n",
        "        self.lora_a = bnb.nn.Int8Params(torch.zeros(weight.size(0), rank), requires_grad=True)\n",
        "        self.lora_b = bnb.nn.Int8Params(torch.zeros(rank, weight.size(1)), requires_grad=True)\n",
        "        torch.nn.init.kaiming_uniform_(self.lora_a)\n",
        "        torch.nn.init.zeros_(self.lora_b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.mm(x, self.weight + torch.mm(self.lora_a.to(torch.float32), self.lora_b.to(torch.float32)))\n",
        "\n",
        "def apply_lora(model, rank):\n",
        "    to_modify = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name and param.requires_grad:\n",
        "            if len(param.size()) == 2:  # Ensure the weight is 2D\n",
        "                to_modify.append((name, param))\n",
        "            else:\n",
        "                print(f\"Skipping {name} as it does not have 2D weight.\")\n",
        "\n",
        "    for name, param in to_modify:\n",
        "        lora_layer = LoRaLayer(param, rank)\n",
        "        setattr(model, name.replace('.', '_'), lora_layer)\n",
        "        param.requires_grad = False\n",
        "\n",
        "def apply_q_lora(model, rank):\n",
        "    to_modify = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name and param.requires_grad:\n",
        "            if len(param.size()) == 2:  # Ensure the weight is 2D\n",
        "                to_modify.append((name, param))\n",
        "            else:\n",
        "                print(f\"Skipping {name} as it does not have 2D weight.\")\n",
        "\n",
        "    for name, param in to_modify:\n",
        "        q_lora_layer = QLoRaLayer(param, rank)\n",
        "        setattr(model, name.replace('.', '_'), q_lora_layer)\n",
        "        param.requires_grad = False\n",
        "\n",
        "def freeze_parameters(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for param in model.transformer.h[-1].parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.lm_head.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "def train_and_evaluate_model(tokenized_train_dataset, tokenized_test_dataset, tokenizer, lora=False, q_lora=False):\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "    if lora:\n",
        "        apply_lora(model, rank=8)\n",
        "    elif q_lora:\n",
        "        apply_q_lora(model, rank=8)\n",
        "    else:\n",
        "        freeze_parameters(model)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=5e-5,\n",
        "        per_device_train_batch_size=1,\n",
        "        per_device_eval_batch_size=1,\n",
        "        num_train_epochs=1,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"loss\",\n",
        "        greater_is_better=False,\n",
        "        fp16=True,\n",
        "        gradient_accumulation_steps=4,\n",
        "        save_safetensors=False  # Disable strict checks for shared tensors\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train_dataset,\n",
        "        eval_dataset=tokenized_test_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    results = trainer.evaluate()\n",
        "    print(\"Evaluation results:\", results)\n",
        "\n",
        "def main(lora=False, q_lora=False):\n",
        "    tokenized_train_dataset, tokenized_test_dataset, tokenizer = load_and_prepare_data()\n",
        "    train_and_evaluate_model(tokenized_train_dataset, tokenized_test_dataset, tokenizer, lora, q_lora)\n",
        "\n",
        "# Set the desired fine-tuning method\n",
        "lora = True  # Set to False if you do not want to use LoRa\n",
        "q_lora = False  # Set to True if you want to use QLoRa\n",
        "\n",
        "# Call the main function with the desired arguments\n",
        "main(lora=lora, q_lora=q_lora)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qZyBzHtgWjSv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}