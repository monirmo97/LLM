# LLM Learning Progress Report

Welcome to my repository where I document my journey and progress in learning Large Language Models (LLMs). This repository serves as a personal learning log, a resource for others, and a platform to share my findings and projects related to LLMs.

## Table of Contents
- [Introduction]
- [Goals]
- [Learning Resources](
- [Progress Log](
- [Projects](
- [Contributing]
- [License]

## Introduction

This repository is dedicated to my exploration and understanding of Large Language Models (LLMs). As a beginner in this field, I aim to grasp the fundamental concepts, techniques, and applications of LLMs through hands-on projects and research.

## Goals

1. **Understand the Basics**: Learn the foundational concepts and architecture of LLMs.
2. **Hands-on Projects**: Implement small projects to apply the concepts learned.
3. **Advanced Topics**: Explore advanced topics such as fine-tuning, prompt engineering, and applications of LLMs in different domains.
4. **Documentation**: Maintain detailed documentation of my learning process and projects.

## Learning Resources

Here are some of the resources I am using to learn about LLMs:

https://www.youtube.com/@AndrejKarpathy
https://www.youtube.com/watch?v=zduSFxRajkE
https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb
https://medium.com/@thakermadhav/build-your-own-rag-with-mistral-7b-and-langchain-97d0c92fa146
https://www.langchain.com/
https://huggingface.co/blog/how-to-generate
Tokenization: https://github.com/openai/openai-cookbook 
Fine-tuning: https: //github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb[first link]
Future: https://github.com/ashishpatel26/LLM-Finetuning It is advanced for fine-tuning after the first link
The below link is for Tokenization, especially for LLm:
https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb 


Feel free to suggest more resources via issues or pull requests!

## Progress Log

I will be updating this section regularly with my progress:

### Week 1 & 2
- **Introduction to LLMs**: Read introductory materials and watched online lectures.
- **Basic Concepts**: Learned about transformers, attention mechanisms, and basic model architectures.

### Week 3
- **tokenization-general-llm**: Read and learn the tokenization practical and theory.
- **Fine-tuning**: fine-tune the first llm model.

## Projects

This section contains the projects I have worked on:
1. **First:** The first project I worked on was fine-tuning the GPT2 on Wikitxt to forcast the next token - Link of project " https://github.com/monirmo97/LLM/tree/main/Train_Test_GPT2"


